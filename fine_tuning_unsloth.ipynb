{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 20px; text-align: center; color: white;\">\n",
    "    <div>\n",
    "        <h1 style=\"margin: 10px 0;\"><strong>Introduction to Fine-Tuning with Unsloth</strong></h1>\n",
    "        <h2>Matthew Sayer, AI Engineer</h2>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color: #ffffff;\"><strong>Part 1:</strong></span> What is Fine-Tuning, and what's Unsloth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fine-Tuning**: An introduction\n",
    "\n",
    "<strong>What's fine-tuning?</strong>\n",
    "\n",
    "Fine-tuning means training an existing model to be able to perform new tasks or to know new things. So, like teaching an old dog new tricks without having it rethink its entire life first.\n",
    "\n",
    "With AI agents, we're often limited by the context window of the major LLMs and can be drowning in token costs. We're building RAG pipelines that bring ridiculous amounts of information into the context of your conversations, which not only slows things down but increases the risk of hallucinations and costs you money.\n",
    "\n",
    "<b>Hint:</b> RAG stands for retrieval-augmented generation. It means that your language model retrieves information from a data source before generating an answer. This grounds your answers to make more reliable and accurate. It's all you need in most use cases when providing knowledge.\n",
    "\n",
    "You don't always need a LARGE language model (LLM). Instead, you can use fine-tuning and RAG to include certain information with a small language model (SLM).\n",
    "\n",
    "This could be a model that knows enough general knowledge to suit your use case, but then has specialised knowledge trained via the fine-tuning process. Your language model doesn't need to know how to cook spaghetti bolognese if its job is to create IT support requests.\n",
    "\n",
    "### <b>Why don't we have to re-train the entire model?</b>\n",
    "\n",
    "You'll often see in the news how much companies such as OpenAI, Anthropic and Grok have spent training their models - I remember hearing that GPT-5 was rumoured to cost 500 million USD per training run.\n",
    "\n",
    "That's an extreme example, but most people don't have anywhere near the hardware needed to train a whole model from scratch - and there's pretty much no point most of the time when you can just fine-tune.\n",
    "\n",
    "##### One of the best ways to do this is with <b>LoRA adapters.</b>\n",
    "\n",
    "LoRA (Low-Rank Adaptation) introduces a small amount of newly-trained parameters into a model, so at different layers within a model we have new knowledge that has been added and is used in inference. It does this by adding low-rank matrices to layers - matrices being an encoded numerical representations of the new knowledge being added.\n",
    "\n",
    "Basically, you're taking an existing brain, and plugging in new modules to it without changing the original knowledge. Imagine you were exactly the same person, but someone injected a chip into your brain that lets you do something new or suddenly be an expert on a certain topic.\n",
    "\n",
    "### **Unsloth**: An introduction\n",
    "\n",
    "As its name may suggest, it makes training faster. Sort of the opposite of a sloth, I suppose they were going for.\n",
    "\n",
    "It is packaged as a Python library, and gives a way to optimise your model to the bone before it goes into the usual training with the SFTTrainer.\n",
    "\n",
    "\n",
    "### Why is it better than standard Python model training libraries?\n",
    "1. It can train models around 2x faster than standard libraries.\n",
    "2. Whilst being faster, it can use 70% less VRAM and that makes it far more likely for your standard laptop or PC to be able to run it.\n",
    "3. Simpler optimisation - it abstracts a lot of the complexity of optimising your config before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color: #ffffff;\"><strong>Part 2:</strong></span> Setup and Run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No setup needed!\n",
    "\n",
    "Go straight to my Google Colab notebook here: https://colab.research.google.com/drive/159VUtuOFFsDqcDnm0Ezm5hyUWUzG586W?usp=sharing\n",
    "\n",
    "### NOTE: MAKE SURE TO UPLOAD THE KENT.JSONL dataset file to your files in the Colab notebook before you run it (stored in this project's root). You should be able to run it for free using a T4 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Author: Matthew Sayer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
